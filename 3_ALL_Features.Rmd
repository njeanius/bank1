---
title: "Part 3 - Feature Selection Methods"
author: "jean wills"
date: "07/07/2020"
output: word_document
---

# Step 3 - Feature Selection Methods
Note that these models are NOT being tested for metrics, but rather as a 'one-off" to see if we can reduce the number of attributes

### 1: PCA
### 2. Recursive feature elimination (RFE) with Random Forest
### 3. Boruta 
### 4. neural network LVQ method
### 5. Regression - ANOVA, AIC
### 6. Regression - AIC stepwise

## this file uses "BM_mini_num_sc"

## Get the data in the proper format first:

### read in dataset - on "BM_mini"

```{r}
library(plyr)
library(dplyr)
BM_mini <- read.csv("/Users/jeanwills/Desktop/CKME136/1_data/bank.csv", header=T, sep = ";", stringsAsFactors = T, na.strings = "NA")
```

### NUMERIC DATA Cleaning - change numeric data to 95% CI 

```{r}
BM<-BM_mini

Lq_bal<- quantile(BM$balance, probs=c(0.025))
Hq_bal<- quantile(BM$balance, probs=c(0.975))
Lq_dur<- quantile(BM$duration, probs=c(0.025))
Hq_dur<- quantile(BM$duration, probs=c(0.975))
Lq_cam<- quantile(BM$campaign, probs=c(0.025))
Hq_cam<- quantile(BM$campaign, probs=c(0.975))
Lq_days<- quantile(BM$pdays, probs=c(0.025))
Hq_days<- quantile(BM$pdays, probs=c(0.975))
Lq_prv<- quantile(BM$previous, probs=c(0.025))
Hq_prv<- quantile(BM$previous, probs=c(0.975))
BM$balance[BM$balance < Lq_bal] <- Lq_bal
BM$balance[BM$balance > Hq_bal] <- Hq_bal
BM$duration[BM$duration < Lq_dur] <- Lq_dur
BM$duration[BM$duration > Hq_dur] <- Hq_dur
BM$campaign[BM$campaign < Lq_cam] <- Lq_cam
BM$campaign[BM$campaign > Hq_cam] <- Hq_cam
BM$pdays[BM$pdays < Lq_days] <- Lq_days
BM$pdays[BM$pdays > Hq_days] <- Hq_days
BM$previous[BM$previous < Lq_prv] <- Lq_prv
BM$previous[BM$previous > Hq_prv] <- Hq_prv

# now have file BM ...
```

### now make minor adjsutments

```{r}
# switch -1 -> 0 in 'pdays'
BM$pdays<- ifelse(BM$pdays == -1, 0, BM$pdays)
# switch duration in seconds to minutes for easier use
BM$duration<- BM$duration/60
```

### now switch categorical to get numeric - RESULT is "BM_mini_num"

```{r}
# numeric still NOT normalized/scaled, 
# actually ALL DATA not normalized/scaled EXCEPT default, housing, loan
BM_n <- BM
BM_n$job<- as.numeric(BM_n$job)  #12
# marital: 1-single, 2-married, 3-divorced
BM_n$marital<- ifelse(BM_n$marital == c("single"), 1, 
                        ifelse(BM_n$marital== c("married"), 2, 3))
# education: 0:unknown, 1: primary, 2:secondary, 3:divorced
BM_n$education<- ifelse(BM_n$education == c("unknown"), 0, 
                          ifelse(BM_n$education == c("primary"), 1, 
                                 ifelse(BM_n$education == c("secondary"), 2, 3)))
# default, housing, loan: if yes then 0 else 1
# BM_num$housing<- as.numeric(BM_num$housing)  #2
BM_n$default<- ifelse(BM_n$default == c("yes"), 0, 1) #2
BM_n$housing<- ifelse(BM_n$housing == c("yes"), 0, 1) #2
BM_n$loan<- ifelse(BM_n$loan == c("yes"), 0, 1) #2
BM_n$contact<- as.numeric(BM_n$contact) #3
# month: jan:1, feb:2.....dec:12
BM_n$month<- ifelse(BM_n$month == "jan", 1, 
                     ifelse(BM_n$month == "feb", 2, 
                            ifelse(BM_n$month == "mar", 3,
                                   ifelse(BM_n$month == "apr", 4, 
                                          ifelse(BM_n$month == "may", 5, 
                                                 ifelse(BM_n$month == "jun", 6,
                                                        ifelse(BM_n$month == "jul", 7,
                      ifelse(BM_n$month == "aug", 8,
                            ifelse(BM_n$month == "sep", 9,
                                   ifelse(BM_n$month == "oct", 10,
                                          ifelse(BM_n$month == "nov", 11, 12)))))))))))
# poutcome: 0:unknown,other, 1:failure, 2: success
BM_n$poutcome<- ifelse(BM_n$poutcome == c("failure"), 1, ifelse(BM_n$poutcome== c("success"), 2, 0))  
# result: BM_num with only numeric data (NOT scaled)
# extra step for BM_mini
#  BM_n

```

#
### scale only original numeric data 



```{r}
# KEEP: age-1, balance-6, day-10, duration-12, campaign-13, pdays-14, previous-15
BMS<- BM_n
# BMS<-BM_mini
normalize<- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
BM_s<- as.data.frame(lapply(BMS[,c(1,6,10,12:15)], normalize))
# now recombine dataframes with the nominal components
BM_s$job<-BMS$job
BM_s$marital<-BMS$marital
BM_s$education<-BMS$education
BM_s$default<-BMS$default
BM_s$housing<-BMS$housing
BM_s$loan<-BMS$loan
BM_s$contact<-BMS$contact
BM_s$month<-BMS$month
BM_s$poutcome<-BMS$poutcome
BM_s$y<-BMS$y
# convert
BM_mini_sc<-BM_s
# result used BM_num file but now BM_num_scale with normalized numeric data
# and y is factor
# to convert y to numeric use next line
# BM_scale$y<- ifelse(BM_scale$y==c("yes"), 1, 0)
rm(BMS)
rm(BM_s)
rm(BM)
rm(BM_n)
```


## we are using SMOTE since it was found to be better so data will be rebalanced with SMOTE first

# step 2 - run training and test datasets FOR ALL CONFIGURATIONS

```{r}
# str(BM_mini_sc)
# rename datasets
BM<-BM_mini_sc
set.seed(30)
# get train and test datasets
# note that if we use cross validation, we can use the complete dataset for training or keep a portion for validation after
#
BM_train_index <- sample(nrow(BM), 0.7 * nrow(BM))
BM_train<- BM[BM_train_index, ]
BM_test <- BM[-BM_train_index, ]
BM_train_labels <- BM[BM_train_index, 17]
BM_test_labels <- BM[-BM_train_index, 17]
# note that we only balance the training sets
# and leave test set as is.
```

# step 3a - run SPECIFIC Balancing step to get balanced data version:

## SMOTE SAMPLE MAJOR/MINOR -  each for training

```{r}
# install.packages("DMwR")
library(DMwR)
library(grid)
# this one is slow
set.seed(50)
smote_train <- SMOTE(y ~ ., data  = BM_train)                         
table(smote_train$y) 

# now we name datasets that are used in models below
train=smote_train

BMX<- smote_train
data<- smote_train
RFEdata<-smote_train
BMX<-smote_train
dataset<- smote_train
# train<- smote_train
pca_trainset = smote_train 
BMY<- smote_train
```


## #1 - PCA

Principal component analysis (PCA) is a technique for reducing the dimensionality of datasets. It does so by creating new uncorrelated variables that maximize variance. There is no linearity or normality assumed in PCA. It seems that if the original correlation coeff's between data < 0.3 then PCA won't work well.

```{r}
# data s/b numeric
# do PCA on the independent variables only - take out $y
BMX<- BMX[,-17]
pc_BM<- princomp(BMX)
# pc_BM<- princomp(BMX)
# pc_BM$scores
summary(pc_BM)
```

## visually see the data...

```{r}
# can plot
plot(pc_BM)
# scree plot
screeplot(pc_BM, type="line", main="Scree Plot")
# see actual component values
pc_BM$loadings
```

### PCA version 2 - This version uses "prcomp"
The code below shows that we still require ~10 princ.comp.'s to explain about 80% of the results. And each component is not responsible for a high percentageof the output. SO this PCA version not very useful on this dataset.

```{r}
# PCA version 2
# this example does not use test set....
# convert y to numeric()
# to convert y to numeric use next line
pca_trainset$y<- ifelse(pca_trainset$y==c("yes"), 1, 0)
pca = prcomp( pca_trainset, scale = T )
# variance
pr_var = ( pca$sdev )^2 

# % of variance
prop_varex = pr_var / sum( pr_var )

# Plot
plot( prop_varex, xlab = "Principal Component", 
                  ylab = "Proportion of Variance Explained", type = "b" )
# different scree plot
# Scree Plot
plot( cumsum( prop_varex ), xlab = "Principal Component", 
                            ylab = "Cumulative Proportion of Variance Explained", type = "b" )
```

###  #2  Recursive Feature Elimination with Random Forest

Recursive feature elimination (RFE) is a feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached. A Random Forest algorithm is used on each iteration to evaluate the model. Method = cross-validation, number=10-fold, repeated 10 times.


```{r}
set.seed(7)
library(mlbench)
library(caret)
# library(lattice)
# library(ggplot2)
# define the control using a random forest selection function rfFuncs
control <- rfeControl(functions=rfFuncs, method="repeatedcv", number=10, repeats=10)
# run the RFE algorithm
results <- rfe(RFEdata[,1:16], RFEdata[,17], sizes=c(1:16), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
```

### #3 Boruta

```{r}
# y is factor with 2 variables yes or no
library(Boruta)
library(mlbench)
library(caret)
library(ranger)
# build classification model
set.seed(111)
boruta<- Boruta(y ~ ., data = BMY, pValue = 0.01, mcAdj = TRUE, maxRuns = 200, doTrace = 0, holdHistory = TRUE, getImp = getImpRfZ)
print(boruta)
# x=predictors, y is response vector, pValue is Conf.level, mcAdj=TRUE uses Bonferroni method, 
# maxRuns can be increased to resolve attributes left 'Tentative'.
# getImp=getImpRfZ, which runs random forest from the ranger package and gathers Z-scores of mean decrease accuracy measure.
# And is the function used to obtain attribute importance.
```

# Plot boruta

```{r}
plot(boruta, las=2, cex.axis = 0.7)
# green are important attributes, red are not, and yellow are tentative
# example: duration is very green, and job is red
```

##  #4  LVQ method

```{r}
# NEW second set: size=50, k=10
# y stays as a category
library(parallel)
library(doMC)
library(mlbench)
library(caret)
library(class)
set.seed(7)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=10)
# manual example: 
# grid<- expand.grid(size=c(5,10,20,50), k=c(1,2,4,5,10)) 
grid<- expand.grid(size=c(25,50), k=10) 
model <- train(y~., data=dataset, method="lvq", tuneGrid=grid, trControl=control)
print(model)
plot(model)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
print(importance)
plot(importance)
```


## model 5 - MULTIPLE REGRESSION - running only once and not using test dataset

```{r}
# let's build a set of models of various combo's of attributes and use ANOVA or AIC to choose best model
# now convert y to 0 or 1 
train$y<- ifelse(train$y==c("yes"), 1, 0)

m1<- lm(y~ contact, data=train)
m2<- lm(y~ contact + poutcome, data=train)
m3<- lm(y~ duration + poutcome, data=train)
m4<- lm(y~ poutcome + month, data=train)
m5<- lm(y~ poutcome, data=train)

m6<- lm(y~ poutcome + month + job, data=train)
m7<- lm(y~ month, data=train)
m8<- lm(y~ education, data=train)
m9<- lm(y~ job, data=train)
m10<- lm(y~ poutcome + month + contact + default + education + job + duration, data=train)

m11<- lm(y~ month + contact + default + education + job + duration, data=train)
m12<- lm(y~ poutcome + month + contact + default + job + duration, data=train)
m13<- lm(y~ poutcome + month + contact + job + duration, data=train)
m14<- lm(y~ poutcome + month + contact + duration, data=train)
m15<- lm(y~ poutcome + month + job + duration, data=train)

m16<- lm(y~ poutcome + month +  default  + duration, data=train)
m17<- lm(y~ poutcome + month + duration, data=train)
m18<- lm(y~ poutcome + job + duration, data=train)
m19<- lm(y~ poutcome + contact + duration, data=train)
m20<- lm(y~ poutcome + month + contact + default + education + job, data=train)

m21<- lm(y~ poutcome + month + contact + default + education, data=train)
m22<- lm(y~ poutcome + month + contact + default, data=train)
m23<- lm(y~ poutcome + month + contact, data=train)
m24<- lm(y~ duration, data=train)
m25<- lm(y~ contact + default + education, data=train)

m26<- lm(y~ contact + default, data=train)
m27<- lm(y~ poutcome + month + contact, data=train)
m28<- lm(y~ poutcome + contact + job, data=train)
#
# run anova on all the models and compare RSS and pick lowest (of those that are significant)
anova(m1,m2,m3,m4,m5,m6,m7,m8,m9,m10,m11,m12,m13,m14,m15,m16,m17,m18,m19,m20,m21,m22,m23,m24,m25,m26,m27,m28)
```

## Run AIC model version 

```{r}
AIC(m1,m2,m3,m4,m5,m6,m7,m8,m9,m10,m11,m12,m13,m14,m15,m16,m17,m18,m19,m20,m21,m22,m23,m24,m25,m26,m27,m28)
```

## Run stepwise model 

```{r}
nullModel<-lm(y~1,data=train) # start with 0
fullModel<-lm(y~., data=train) # try ALL
houseStep<- step(nullModel, scope=list(lower=nullModel, upper=fullModel), direction = "both")
```

### use command to see step results

```{r}
houseStep
```




